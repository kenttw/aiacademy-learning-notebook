{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Boosting 參考資料\n",
    "https://medium.com/@cwchang/gradient-boosting-%E7%B0%A1%E4%BB%8B-f3a578ae7205\n",
    "主要概念如下\n",
    "* 將一堆的 weak leanner 給予串連起，進而組出更強的 model\n",
    "* 每一個 learner 各有強項，故針對不同的資料，都要有專門 learner 來功克。\n",
    "* 下一個 learner 必需要要補強上一個 learner 無法有效處理的\n",
    "* 故針對每一個 instance(record) 都要附予不同的 weight ，而這 weighting 的依據是來自於上一個 learner 所以學不好的地方。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/zA9337p.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting背後的演算法\n",
    "以AdaBoost為例，我們可以看到，假設我們原本的訓練資料集裡面有m筆資料，那麼一開始我們每筆樣本被抽到的機率就是\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mo>&#x8A13;&#x7DF4;&#x96C6;</mo>\n",
    "  <mi>S</mi>\n",
    "  <mo>=</mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>y</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>,</mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>y</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x2026;</mo>\n",
    "  <mo>,</mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mi>m</mi>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>y</mi>\n",
    "    <mi>m</mi>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mo>&#x521D;&#x59CB;&#x6BCF;&#x500B;&#x6A23;&#x672C;&#x88AB;&#x62BD;&#x5230;&#x7684;&#x6A5F;&#x7387;</mo>\n",
    "  <msup>\n",
    "    <mi>D</mi>\n",
    "    <mrow>\n",
    "      <mo stretchy=\"false\">(</mo>\n",
    "      <mn>1</mn>\n",
    "      <mo stretchy=\"false\">)</mo>\n",
    "    </mrow>\n",
    "  </msup>\n",
    "  <mo>=</mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mfrac>\n",
    "    <mn>1</mn>\n",
    "    <mi>m</mi>\n",
    "  </mfrac>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x2026;</mo>\n",
    "  <mo>,</mo>\n",
    "  <mfrac>\n",
    "    <mn>1</mn>\n",
    "    <mi>m</mi>\n",
    "  </mfrac>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然後我們會進行 T 輪的迭代學習\n",
    "\n",
    "在每一次迭代，我們都會根據現在樣本抽到機率使用弱學習器進行學習，進而得到這次迭代的模型，然後再去計算這次迭代模型的誤差，然後更新每個樣本被抽到的機率，然後準備進行下一輪。 其詳細的演算法可觀看下圖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/cJTmrzr.png' widht=10% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
